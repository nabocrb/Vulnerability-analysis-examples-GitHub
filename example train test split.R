# Vulnerability analysis review
# Train-test split example
# Nathan Bonham
# October 2023

rm(list=ls())

library(here)
library(randomForest)
#library(xgboost)
library(lhs)
library(ggplot2)
library(ggpubr)
# sample and simulate simulate model inputs

blue_back="#082765"
red_back="#d14545"
blue_circle="#4472c4"
red_circle='#c00000'
circ_size=2

set.seed(10)

n=300
inputs=lhs::improvedLHS(n, k=2)
i1=inputs[,1]
i2=inputs[,2]
noise=rnorm(n, sd = .30)
m=i1-i2+noise

# define performance classes

threshold=mean(m)

# train-test split plots

nkeep=0.8*n

train=sample.int(n = n, size = nkeep, replace = F)
test=(1:n)[-train]

plot.df=data.frame(i1,i2, m, 
                   class=ifelse(m > threshold, 'acceptable', 'unacceptable'))
plot.df$train_test='train'
plot.df$train_test[test]='test'

all_points=ggplot(data=plot.df, aes(x=i1, y=i2, fill=class, color=class))+
  geom_point(size=circ_size, shape=19)+
  scale_color_manual(values = c('unacceptable'=red_circle, 'acceptable'=blue_circle))+
  theme_bw()+
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )+
  ggtitle('All simulations')
all_points

plot.df$train_test=factor(plot.df$train_test, levels=c('train', 'test'))

train_test=ggplot(data=plot.df, aes(x=i1, y=i2, fill=class, color=class, shape=train_test))+
  geom_point(size=circ_size, stroke=1.5)+
  scale_color_manual(values = c('unacceptable'=red_circle, 'acceptable'=blue_circle))+
  scale_shape_manual(values=c('train'=19, 'test'=1), name='train/test')+
  theme_bw()+
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.text=element_text(size=12)
  )+ggtitle('Train-test split')+
  scale_y_continuous(expand = c(0,.01))+
  scale_x_continuous(expand = c(0,.01))
  
train_test

ggsave(filename=here('figures', 'train-test points.tif'), device = 'tiff', dpi = 300, width = .6*7, height = .6*4.64)




# fit logistic regression

fitting.df=plot.df[plot.df$train_test=='train',]
fitting.df$binary=ifelse(fitting.df$class=='unacceptable',1,0)

log_reg=glm(binary~i1+i2, data=fitting.df, family = binomial)

# fit randomForest

rf=randomForest(factor(binary)~i1+i2, data=fitting.df)

# 
# # fit xgboost
# 
# training_boosted=data.frame(
#   i1=fitting.df$i1,
#   i2=fitting.df$i2,
#   binary=fitting.df$binary
# )
# 
# trainX=data.frame(i1=fitting.df$i1,
#                   i2=fitting.df$i2)
# 
# 
# 
# boosted=xgboost(data=as.matrix(trainX), label=fitting.df$binary, objective='binary:logistic', nrounds = 100,max.depth=10)

### accuracy calculations

## logistic

# training

yhat=ifelse(predict(log_reg, type='response')>.5, 1, 0)
y=log_reg$y

TA_log=sum(yhat==y)/length(y)

# testing

yhat=ifelse(predict(log_reg, type='response', newdata = plot.df[test,])>.5, 1, 0)
y=ifelse(plot.df$class[test]=='unacceptable',1,0)

PA_log=sum(yhat==y)/length(y)

## Random forest

# training

yhat=ifelse(predict(rf, newdata =plot.df[train,],  type='prob', norm.votes = T)[,2]>.5, 1, 0)
y=ifelse(plot.df$class[train]=='unacceptable',1,0)
  
TA_rf=sum(yhat==y)/length(y)

# testing

yhat=ifelse(predict(rf, newdata =plot.df[test,],  type='prob', norm.votes = T)[,2]>.5, 1, 0)
y=ifelse(plot.df$class[test]=='unacceptable',1,0)

PA_rf=sum(yhat==y)/length(y)

## Boosted trees
# 
# # training
# 
# yhat=ifelse(predict(boosted, as.matrix(trainX))>.5, 1, 0)
# y=ifelse(plot.df$class[train]=='acceptable',1,0)
# 
# TA_boosted=sum(yhat==y)/length(y)
# 
# # testing
# 
# yhat=ifelse(predict(rf,)>.5, 1, 0)
# y=ifelse(plot.df$class[test]=='acceptable',1,0)
# 
# PA_rf=sum(yhat==y)/length(y)

# plotting response surfaces

x1=seq(0,1, length.out=100)
x2=seq(0,1,length.out=100)

newX=expand.grid(i1=x1,i2=x2)

logistic.df=data.frame(
  newX,
  probability=predict(log_reg, newdata = newX, type='response')
)


logistic.training=ggplot(mapping = aes(x=i1, y=i2))+
  geom_raster(data=logistic.df, aes(fill=probability), interpolate = T)+
  scale_fill_gradientn(colours=c(blue_back,'white',red_back), values=c(0,.5,1), name='P(unacceptable)')+
  geom_point(data=plot.df[train,], aes(color=class), size=circ_size)+
  scale_color_manual(values = c('unacceptable'=red_circle, 'acceptable'=blue_circle))+
  scale_shape_manual(values=c('train'=19, 'test'=1))+
  theme_minimal()+
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank(),
    legend.text=element_text(size=12)
  )+
  ggtitle(paste('Accuracy:', round(TA_log,2)))
logistic.training

logistic.prediction=ggplot(mapping = aes(x=i1, y=i2))+
  geom_raster(data=logistic.df, aes(fill=probability), interpolate = T)+
  scale_fill_gradientn(colours=c(blue_back,'white',red_back), values=c(0,.5,1), name='P(unacceptable)')+
  geom_point(data=plot.df[test,], aes(color=class), size=circ_size, shape=1, stroke=1.5)+
  scale_color_manual(values = c('unacceptable'=red_circle, 'acceptable'=blue_circle))+
  #scale_shape_manual(values=c('train'=19, 'test'=1))+
  theme_minimal()+
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank(),
    legend.text=element_text(size=12)
  )+
  ggtitle(paste('Accuracy:', round(PA_log,2)))
logistic.prediction


RF.df=data.frame(
  newX,
  probability=predict(rf, newdata=newX, type='prob', norm.votes = T)[,2]
)

RF.training=ggplot(mapping = aes(x=i1, y=i2))+
  geom_raster(data=RF.df, aes(fill=probability), interpolate = T)+
  scale_fill_gradientn(colours=c(blue_back,'white',red_back), values=c(0,.5,1), name='P(unacceptable)')+
  geom_point(data=plot.df[train,], aes(color=class), size=circ_size)+
  scale_color_manual(values = c('unacceptable'=red_circle, 'acceptable'=blue_circle))+
  scale_shape_manual(values=c('train'=19, 'test'=1))+
  theme_minimal()+
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank(),
    legend.text=element_text(size=12)
  )+
  ggtitle(paste('Accuracy:', round(TA_rf,2)))
RF.training

RF.prediction=ggplot(mapping = aes(x=i1, y=i2))+
  geom_raster(data=RF.df, aes(fill=probability), interpolate = T)+
  scale_fill_gradientn(colours=c(blue_back,'white',red_back), values=c(0,.5,1), name='P(unacceptable)')+
  geom_point(data=plot.df[test,], aes(color=class), size=circ_size, shape=1, stroke=1.5)+
  scale_color_manual(values = c('unacceptable'=red_circle, 'acceptable'=blue_circle))+
 # scale_shape_manual(values=c('train'=19, 'test'=1))+
  theme_minimal()+
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank(),
    legend.text=element_text(size=12)
  )+
  ggtitle(paste('Accuracy:', round(PA_rf,2)))
RF.prediction

plotlist=list(logistic.training, RF.training, logistic.prediction, RF.prediction)

combined=ggarrange(plotlist = plotlist, common.legend = T, legend='right' )

combined

ggsave(filename=here('figures', 'logistic vs RF.tif'), height = .8*6.8, width = .8*8.25, device = 'tiff', dpi = 300, bg = 'white')
